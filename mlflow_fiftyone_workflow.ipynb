{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: #ff6D04; \">MLflow + FiftyOne Workflow</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Guided Walkthrough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "First install the required python libraries below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow fiftyone torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Next we will install the fiftyone-mlflow-plugin that will allow us to view and manage our MLflow client in the FiftyOne App! The App can be run in your browser at localhost:5151 or even in your Databricks Notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/fiftyone_mlflow_plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's kick things off by loading in all of our required libraries. While we are at it, we will start our MLflow client and specifying our `tracking_uri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bson import json_util\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.operators as foo\n",
    "import fiftyone.plugins as fop\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.utils.random as four\n",
    "\n",
    "from fiftyone import ViewField as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_directory = os.path.dirname(fop.find_plugin(\"@jacobmarks/mlflow_tracking\"))\n",
    "if package_directory not in sys.path:\n",
    "    sys.path.append(package_directory)\n",
    "from fiftyone_mlflow_plugin import log_mlflow_run_to_fiftyone_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example workflow, I will be using a subset of the [VisDrone](https://github.com/VisDrone/VisDrone-Dataset?tab=readme-ov-file)dataset, a state of the art drone imagery dataset from  Lab of Machine Learning and Data Mining, Tianjin University, China. It features a wide range of locations, time of day, objects, and angles. The subset we will be using can be downloaded on [Google Drive](https://drive.google.com/file/d/1a2oHjcEcwXP8oUF95qiwrqzACb2YlUhn/view). Once the file is downloaded and unzipped, we can load it in by following our ingestor below!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 6471/6471 [452.5ms elapsed, 0s remaining, 14.3K samples/s]      \n",
      "Computing metadata...\n",
      " 100% |███████████████| 6471/6471 [1.0s elapsed, 0s remaining, 6.4K samples/s]         \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_dir=\"./VisDrone-train/VisDrone2019-DET-train/images\"\n",
    "name = \"VisDrone\"\n",
    "\n",
    "# Create the dataset by loading in the directory of images\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fo.types.ImageDirectory,\n",
    "    name=name,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# We compute the metadata of the dataset to get height and width of all our samples\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VisDrone features 12 different classes which we will create a dictionary for. The annotations are stored as <x, y, w, h, confidence, label, truncation, occlusion> in txt files. Since it is a custom format, we ingest it by looping through our datasets and grabbing each sample. Next we open up the text file and add the detections and all their metadata on a sample by sample basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {0:\"ignore_regions\",\n",
    "             1:\"pedestrians\",\n",
    "             2:\"people\",\n",
    "             3:\"bicycle\",\n",
    "             4:\"car\",\n",
    "             5:\"van\",\n",
    "             6:\"truck\",\n",
    "             7:\"tricycle\",\n",
    "             8:\"awning_tricycle\",\n",
    "             9:\"bus\",\n",
    "             10:\"motor\",\n",
    "             11:\"others\",\n",
    "}\n",
    "\n",
    "ann_dir = \"./VisDrone-train/VisDrone2019-DET-train/annotations/\"\n",
    "\n",
    "for sample in dataset:\n",
    "\n",
    "    # Grab the annotation file\n",
    "    filename = os.path.basename(sample.filepath)\n",
    "    ann = ann_dir + os.path.splitext(filename)[0] + \".txt\"\n",
    "    if os.path.exists(ann):\n",
    "        with open(ann, 'r') as file:\n",
    "            detections = []\n",
    "            for line in file:\n",
    "                split_line = line.strip().split(\",\")\n",
    "                ann_list = [int(x) for x in split_line[:8]]\n",
    "\n",
    "                # Grab all the detection information from the line\n",
    "                label = class_map[ann_list[5]]\n",
    "                trunc = ann_list[6]\n",
    "                occ = ann_list[7]\n",
    "\n",
    "                # FiftyOne takes in normalized (x,y,w,h) bounding boxes\n",
    "                x = ann_list[0] / sample.metadata.width\n",
    "                y = ann_list[1] / sample.metadata.height\n",
    "                w = ann_list[2] / sample.metadata.width\n",
    "                h = ann_list[3] / sample.metadata.height\n",
    "                det = fo.Detection(\n",
    "                    label=label,\n",
    "                    bounding_box = [x,y,w,h],\n",
    "                    truncation=trunc,\n",
    "                    occlusion=occ\n",
    "                )\n",
    "                detections.append(det)\n",
    "\n",
    "            sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "            sample.save()\n",
    "\n",
    "# Set our dataset as persistent\n",
    "dataset.persistent=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading both our images and annotations in, we set the dataset as persistent to have it persist in the database and make sure any new changes will saved. This also allows for easy reloading on future sessions with the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset(\"VisDrone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can launch our FiftyOne app with the line below to visualize our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to FiftyOne on port 5151 at localhost.\n",
      "If you are not connecting to a remote session, you may need to start a new session and specify a port\n",
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "window.open('http://localhost:5151/');",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.open_tab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can begin the data curation process and begin to look for issues or mistakes in our datasets. We can leverage powerful features within FiftyOne to help bring new insights into our dataset and create high quality subsets of our data to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Visualize embeddings with FiftyOne Brain](https://docs.voxel51.com/user_guide/brain.html#visualizing-embeddings)\n",
    "- [Search your datasets with text prompts or sort by similarity](https://docs.voxel51.com/user_guide/brain.html#similarity)\n",
    "- [Find image quality issues](https://github.com/jacobmarks/image-quality-issues)\n",
    "- [Find exact and approximate duplicates](https://github.com/jacobmarks/image-deduplication-plugin)\n",
    "- [Find outliers in your dataset](https://github.com/danielgural/outlier_detection)\n",
    "- [Create interesting views of your dataset by filtering, slicing, sorting, and more!](https://docs.voxel51.com/user_guide/using_views.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these curation tools, the MLFlow panel and more are powered by [FiftyOne Plugins](https://github.com/voxel51/fiftyone-plugins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created a view you like, we need to export the dataset in YOLO format in order to train YOLO9. We do so by randomly splitting and using the `export` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1779/1779 [12.3s elapsed, 0s remaining, 126.8 samples/s]      \n",
      "Directory 'VisDrone_curated/' already exists; export will be merged with existing files\n",
      " 100% |███████████████| 6308/6308 [43.9s elapsed, 0s remaining, 91.8 samples/s]       \n",
      "Directory 'VisDrone_curated/' already exists; export will be merged with existing files\n",
      " 100% |█████████████████████| 0/0 [6.2ms elapsed, ? remaining, ? samples/s] \n"
     ]
    }
   ],
   "source": [
    "class_map = {0:\"ignore_regions\",\n",
    "             1:\"pedestrians\",\n",
    "             2:\"people\",\n",
    "             3:\"bicycle\",\n",
    "             4:\"car\",\n",
    "             5:\"van\",\n",
    "             6:\"truck\",\n",
    "             7:\"tricycle\",\n",
    "             8:\"awning_tricycle\",\n",
    "             9:\"bus\",\n",
    "             10:\"motor\",\n",
    "             11:\"others\",\n",
    "}\n",
    "\n",
    "# Replace below with you own saved view, or use the whole dataset\n",
    "#curated = dataset.load_saved_view(\"Curated\")\n",
    "curated = dataset\n",
    "\n",
    "four.random_split(curated, {\"val\": 0.15, \"train\": 0.85})\n",
    "classes = list(class_map.values())\n",
    "\n",
    "for split in [\"val\",\"train\",\"test\"]:\n",
    "    view =  curated.match_tags(split)\n",
    "    view.export(\n",
    "        export_dir=\"VisDrone_curated/\",\n",
    "        split=split,\n",
    "        dataset_type=fo.types.YOLOv5Dataset,\n",
    "        classes=classes\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get started, we will be training with Ultralytics YOLOv9. We will take advantage of the Ultralytics MLflow integration to round out our stack for this workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we define some helper functions that help us check to see if an experiment exists on our dataset, and if it does not, create a new one with a serialized version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_view(view):\n",
    "    \"\"\"\n",
    "    Returns a serilized verision of a view in a json dump\n",
    "\n",
    "    Args:\n",
    "    - view: The name of the view to be serialized\n",
    "    \"\"\"\n",
    "    return json.loads(json_util.dumps(view._serialize()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_exists(experiment_name):\n",
    "    \"\"\"\n",
    "    Checks to see if an experiment exists already\n",
    "\n",
    "    Args:\n",
    "    - experiment_name: The name of the MLflow experiment to check\n",
    "    \"\"\"\n",
    "    return mlflow.get_experiment_by_name(experiment_name) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fiftyone_mlflow_experiment(\n",
    "    experiment_name, sample_collection, experiment_description=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a new MLflow experiment for a FiftyOne sample collection.\n",
    "\n",
    "    Args:\n",
    "    - experiment_name: The name of the MLflow experiment to create\n",
    "    - sample_collection: A FiftyOne sample collection to use as the dataset for the experiment\n",
    "    - experiment_description: An optional description for the MLflow experiment\n",
    "    \"\"\"\n",
    "\n",
    "    tags = {\n",
    "        \"mlflow.note.content\": experiment_description,\n",
    "        \"dataset\": sample_collection._dataset.name,\n",
    "    }\n",
    "    client.create_experiment(name=experiment_name, tags=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we define our core `run_fiftyone_mlflow_experiment` function. This will allow us to pass in our FiftyOne dataset or view and begin a training run. The run will be stored on MLFlow with information of the hyperparameters, dataset contents, and metrics during training like mAP score! A custom run will also be saved to the FiftyOne dataset that saves information like the tracking_uri and experiment name from MLFlow! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fiftyone_mlflow_experiment(\n",
    "    sample_collection,\n",
    "    training_func,\n",
    "    experiment_name,\n",
    "    experiment_description=\"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run an MLFlow experiment on a FiftyOne sample collection using the provided model and training function.\n",
    "\n",
    "    Args:\n",
    "    - sample_collection: A FiftyOne sample collection to use as the dataset for the experiment\n",
    "    - training_func: A function that trains the model and returns it\n",
    "    - experiment_name: The name of the MLflow experiment to create\n",
    "    - experiment_description: An optional description for the MLflow experiment\n",
    "    \"\"\"\n",
    "\n",
    "    if not experiment_exists(experiment_name):\n",
    "        create_fiftyone_mlflow_experiment(\n",
    "            experiment_name, sample_collection, experiment_description\n",
    "        )\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    \n",
    "    # Build a YOLOv9c model from pretrained weight\n",
    "    model = YOLO('yolov9c.pt')\n",
    "    \n",
    "    # Display model information (optional)\n",
    "    model.info()\n",
    "    \n",
    "    # Train the model on the COCO8 example dataset for 100 epochs\n",
    "    results = training_func(\n",
    "        data='../VisDrone_curated/dataset.yaml',\n",
    "        epochs=2,\n",
    "        imgsz=640,\n",
    "        batch=4,\n",
    "        project=experiment_name,\n",
    "        name=\"curated\"\n",
    "    )\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we pass in our dataset or view, our training function, and the name of the experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv9c summary: 618 layers, 25590912 parameters, 0 gradients, 104.0 GFLOPs\n",
      "YOLOv9c summary: 618 layers, 25590912 parameters, 0 gradients, 104.0 GFLOPs\n",
      "New https://pypi.org/project/ultralytics/8.1.25 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.24 🚀 Python-3.9.18 torch-2.2.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4080 Laptop GPU, 12010MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov9c.pt, data=../VisDrone_curated/dataset.yaml, epochs=2, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=mlflow_fiftyone, name=test3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=mlflow_fiftyone/test3\n",
      "Overriding model.yaml nc=80 with nc=12\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    212864  ultralytics.nn.modules.block.RepNCSPELAN4    [128, 256, 128, 64, 1]        \n",
      "  3                  -1  1    164352  ultralytics.nn.modules.block.ADown           [256, 256]                    \n",
      "  4                  -1  1    847616  ultralytics.nn.modules.block.RepNCSPELAN4    [256, 512, 256, 128, 1]       \n",
      "  5                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      "  6                  -1  1   2857472  ultralytics.nn.modules.block.RepNCSPELAN4    [512, 512, 512, 256, 1]       \n",
      "  7                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      "  8                  -1  1   2857472  ultralytics.nn.modules.block.RepNCSPELAN4    [512, 512, 512, 256, 1]       \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPELAN         [512, 512, 256]               \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1   3119616  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 512, 512, 256, 1]      \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    912640  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 256, 256, 128, 1]      \n",
      " 16                  -1  1    164352  ultralytics.nn.modules.block.ADown           [256, 256]                    \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1   2988544  ultralytics.nn.modules.block.RepNCSPELAN4    [768, 512, 512, 256, 1]       \n",
      " 19                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   3119616  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 512, 512, 256, 1]      \n",
      " 22        [15, 18, 21]  1   5592052  ultralytics.nn.modules.head.Detect           [12, [256, 512, 512]]         \n",
      "YOLOv9c summary: 618 layers, 25538484 parameters, 25538468 gradients, 103.7 GFLOPs\n",
      "\n",
      "Transferred 931/937 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir mlflow_fiftyone/test3', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/dan/Documents/databricks/VisDrone_curated/labels/train.cache... 6308 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6308/6308 [00:00<?, ?it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/dan/Documents/databricks/VisDrone_curated/images/train/0000137_02220_d_0000163.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/dan/Documents/databricks/VisDrone_curated/images/train/0000140_00118_d_0000002.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/dan/Documents/databricks/VisDrone_curated/images/train/9999945_00000_d_0000114.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/dan/Documents/databricks/VisDrone_curated/images/train/9999987_00000_d_0000049.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/dan/Documents/databricks/VisDrone_curated/images/train/9999998_00219_d_0000175.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/dan/Documents/databricks/VisDrone_curated/labels/val.cache... 1779 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1779/1779 [00:00<?, ?it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/dan/Documents/databricks/VisDrone_curated/images/val/0000137_02220_d_0000163.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to mlflow_fiftyone/test3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000625, momentum=0.9) with parameter groups 154 weight(decay=0.0), 161 weight(decay=0.0005), 160 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/09 13:25:23 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of transformers. If you encounter errors during autologging, try upgrading / downgrading transformers to a supported version, or try upgrading MLflow.\n",
      "2024/03/09 13:25:23 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n",
      "2024/03/09 13:25:23 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n",
      "2024/03/09 13:25:23 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(bd485da68c654dc690c2b8494229f8f0) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mmlflow_fiftyone/test3\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      4.87G      1.428      1.335     0.9815        320        640: 100%|██████████| 1577/1577 [02:30<00:00, 10.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 223/223 [00:11<00:00, 19.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1779      97275      0.422      0.268      0.256       0.15\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2      4.48G       1.35      1.055     0.9522        193        640: 100%|██████████| 1577/1577 [02:17<00:00, 11.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 223/223 [00:11<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1779      97275      0.488      0.309      0.302      0.178\n",
      "\n",
      "2 epochs completed in 0.088 hours.\n",
      "Optimizer stripped from mlflow_fiftyone/test3/weights/last.pt, 51.6MB\n",
      "Optimizer stripped from mlflow_fiftyone/test3/weights/best.pt, 51.6MB\n",
      "\n",
      "Validating mlflow_fiftyone/test3/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.24 🚀 Python-3.9.18 torch-2.2.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4080 Laptop GPU, 12010MiB)\n",
      "YOLOv9c summary (fused): 384 layers, 25328500 parameters, 0 gradients, 102.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 1/223 [00:00<00:22,  9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 223/223 [00:19<00:00, 11.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1779      97275      0.486      0.309      0.301      0.178\n",
      "        ignore_regions       1779       2342          1          0     0.0155    0.00534\n",
      "           pedestrians       1779      20413      0.585      0.337       0.38      0.171\n",
      "                people       1779       7698      0.377      0.163      0.157     0.0517\n",
      "               bicycle       1779       2871      0.393      0.113      0.118     0.0509\n",
      "                   car       1779      40695      0.632      0.687       0.69      0.447\n",
      "                   van       1779       6980      0.473      0.449       0.44      0.304\n",
      "                 truck       1779       3429      0.521      0.476      0.463      0.313\n",
      "              tricycle       1779       1236      0.438      0.221        0.2      0.116\n",
      "       awning_tricycle       1779        835      0.313      0.268      0.196      0.121\n",
      "                   bus       1779       1667      0.487      0.602      0.584      0.398\n",
      "                 motor       1779       8653      0.425      0.337      0.308       0.13\n",
      "                others       1779        456      0.184     0.0592     0.0621     0.0266\n",
      "Speed: 0.1ms preprocess, 3.3ms inference, 0.0ms loss, 4.6ms postprocess per image\n",
      "Results saved to \u001b[1mmlflow_fiftyone/test3\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    }
   ],
   "source": [
    "# Build a YOLOv9c model from pretrained weight\n",
    "model = YOLO('yolov9c.pt')\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()\n",
    "\n",
    "run_fiftyone_mlflow_experiment(dataset,model.train, \"mlflow_fiftyone\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our run, we can monitor its status in the FiftyOne App through the MLFlow panel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/mlflow.gif\" alt=\"MLFLow Monitoring\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Predictions to Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/dan/Documents/databricks/VisDrone-train/VisDrone2019-DET-train/images/0000002_00005_d_0000014.jpg: 384x640 6 peoples, 53 cars, 2 vans, 1 tricycle, 6 motors, 7.6ms\n",
      "Speed: 12.9ms preprocess, 7.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "sample = dataset.first()\n",
    "result = model(sample.filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4., 10.,  4.,  4.,  2.,  4.,  4.,  4., 10.,  4.,  4.,  4.,  4.,  2.,  4.,  4.,  5.,  4.,  4.,  4.,  4.,  4.,  4.,  4., 10.,  5.,  2.,  7.,  4.,  4.,  2., 10.,  4., 10.,  4.,\n",
       "         4.,  2., 10.,  2.,  4.,  4.], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].boxes.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].boxes.cpu().numpy().cls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To begin adding predictions to our dataset, we load in our model from a checkpoint with the code below if needed. If the model is still loaded after training we can continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO('mlflow_fiftyone/curated/weights/best.pt')  # load from training run if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we can just pass our Ultralytics YOLOv9 model to `apply_model` to add detections to all of our samples!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||--------------|    0/6471 [12.1ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 6471/6471 [4.0m elapsed, 0s remaining, 17.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset.apply_model(model, label_field=\"predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's view our dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use `evaluate_detections` and calculate the mAP of our model. We also add metadata to our sample detections such if they were a false potive or a true positive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |███████████████| 6471/6471 [20.0m elapsed, 0s remaining, 6.6 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 6471/6471 [6.8m elapsed, 0s remaining, 14.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "results = dataset.evaluate_detections(pred_field=\"predictoins\", gt_field=\"ground_truth\", eval_key=\"eval\", compute_mAP=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can repeat the workflow of adding predictions and evaluating for any number of models on our dataset! You can even compare predicitions from one model to another using the [model comparision](https://github.com/allenleetc/model-comparison) plugin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/model_compare_input.gif\" alt=\"Model Compare Input\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can choose from a variety of options to see exactly where your two models differ. Forget searching across hundreds of thousands of detection, the model comparision plugin will bring only the samples of interest right in front of you! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/model_compare_out.gif\" alt=\"Model Compare Input\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A trained model can also help use during data curation! One of the most common ways is to check your high confidence false postives. This is where you are most likely to find annotation mistakes in your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/high_cf_fp.gif\" alt=\"High Conf False Positives\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
